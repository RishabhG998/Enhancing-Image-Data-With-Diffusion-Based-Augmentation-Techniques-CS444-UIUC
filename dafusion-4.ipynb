{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qvxk4vldswWi","executionInfo":{"status":"ok","timestamp":1684008724563,"user_tz":300,"elapsed":20415,"user":{"displayName":"Shrey Sarswat","userId":"00735471870348438365"}},"outputId":"bb6848c9-b112-4186-9e06-f1adc5d79c5f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"]},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/DLCV/project/da-fusion/')"],"metadata":{"id":"fhnFP15as6Lp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip pascal.zip\n","# !git clone https://github.com/brandontrabucco/da-fusion.git"],"metadata":{"id":"MuTuMq1CtIOl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install diffusers[\"torch\"] transformers pycocotools pandas matplotlib seaborn scipy datasets"],"metadata":{"id":"EuBZW9FVta2M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -e da-fusion"],"metadata":{"id":"RiETvepYtoeX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !git clone https://github.com/CompVis/stable-diffusion.git"],"metadata":{"id":"oGb09Q_mtuer"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -e stable-diffusion/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oX0BMJ1qt9Nf","executionInfo":{"status":"ok","timestamp":1683844541315,"user_tz":300,"elapsed":2705,"user":{"displayName":"Shrey Sarswat","userId":"00735471870348438365"}},"outputId":"9711b152-7f2e-42a0-81b5-7592c7e3b7f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing collected packages: latent-diffusion\n","  Attempting uninstall: latent-diffusion\n","    Found existing installation: latent-diffusion 0.0.1\n","    Uninstalling latent-diffusion-0.0.1:\n","      Successfully uninstalled latent-diffusion-0.0.1\n","  Running setup.py develop for latent-diffusion\n","Successfully installed latent-diffusion-0.0.1\n"]}]},{"cell_type":"code","source":["!python train_classifier.py --logdir pascal-baselines/textual-inversion-0.5 \\\n","--synthetic-dir \"aug/textual-inversion-0.5/{dataset}-{seed}-{examples_per_class}\" \\\n","--dataset pascal --prompt \"a photo of a {name}\" \\\n","--aug textual-inversion --guidance-scale 7.5 \\\n","--strength 0.5 --mask 0 --inverted 0 \\\n","--num-synthetic 10 --synthetic-probability 0.5 \\\n","--num-trials 1 --examples-per-class 4"],"metadata":{"id":"8o8yJvoLuDyh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip /content/drive/MyDrive/DLCV/project/da-fusion/data/pascal.zip"],"metadata":{"id":"_YTJS70Ov_hr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","import numpy as np\n","import time\n","import tqdm\n","import shutil\n","import torch.nn as nn\n","import torchvision.models as  models\n","from torch.utils.data import DataLoader, ConcatDataset\n","import torchvision.datasets.voc as voc\n","import torch.optim as optim\n","from PIL import Image\n","from glob import glob\n","from torchvision import transforms\n","from torchvision.models import resnet18\n","import torch.utils.model_zoo as model_zoo\n","import xml.etree.cElementTree as ET"],"metadata":{"id":"ualFm9Jw8F4o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_dir = '/content/drive/MyDrive/DLCV/project/da-fusion/data'\n","aug_dir = '/content/drive/MyDrive/DLCV/project/da-fusion/pascal-aug'\n","ckpt_dir = '/content/drive/MyDrive/cs444/project/checkpoints'\n","object_categories = ['aeroplane', 'bicycle', 'bird', 'boat',\n","                     'bottle', 'bus', 'car', 'cat', 'chair',\n","                     'cow', 'diningtable', 'dog', 'horse',\n","                     'motorbike', 'person', 'pottedplant',\n","                     'sheep', 'sofa', 'train', 'tvmonitor']\n","num_classes = len(object_categories)\n","batch_size = 32\n","resnet_lr = 1e-5\n","fc_lr = 5e-3\n","num_epochs = 35\n","\n","mean = [0.457342265910642, 0.4387686270106377, 0.4073427106250871]\n","std = [0.26753769276329037, 0.2638145880487105, 0.2776826934044154]\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","np.random.seed(1902)\n","torch.manual_seed(1902)"],"metadata":{"id":"FbIVn3Eg8H6H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684011957234,"user_tz":300,"elapsed":2,"user":{"displayName":"Shrey Sarswat","userId":"00735471870348438365"}},"outputId":"4e240842-cc4f-4ec1-c8ca-ba86137d9125"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7ff69e76a970>"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["class PascalVOC_Dataset(voc.VOCDetection):\n","    \"\"\"Pascal VOC Detection Dataset\"\"\"\n","    def __init__(self, root, image_set='train', download=False, transform=None, target_transform=None):\n","        super().__init__(root, image_set=image_set, download=download, transform=transform, target_transform=target_transform)\n","    \n","    def __getitem__(self, index):\n","        return super().__getitem__(index)\n","    \n","    def __len__(self):\n","        return len(self.images)"],"metadata":{"id":"sF-T92GPrVJa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def encode_labels(target):\n","    \"\"\"Encode multiple labels using 1/0 encoding\"\"\"\n","    ls = target['annotation']['object']\n","    j = []\n","    if type(ls) == dict:\n","        if int(ls['difficult']) == 0:\n","            j.append(object_categories.index(ls['name']))\n","    else:\n","        for i in range(len(ls)):\n","            if int(ls[i]['difficult']) == 0:\n","                j.append(object_categories.index(ls[i]['name']))\n","    k = np.zeros(len(object_categories))\n","    k[j] = 1\n","    return torch.from_numpy(k)"],"metadata":{"id":"Ct-8GY86rX6f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transformations = transforms.Compose([transforms.Resize((300, 300)),\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize(mean=mean, std=std)])\n","transformations_valid = transforms.Compose([transforms.Resize(330), \n","                                            transforms.CenterCrop(300), \n","                                            transforms.ToTensor(),\n","                                            transforms.Normalize(mean=mean, std=std)])"],"metadata":{"id":"5EyN-C2oradM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_train = PascalVOC_Dataset(data_dir,\n","                                  image_set='train', \n","                                  download=False, \n","                                  transform=transformations, \n","                                  target_transform=encode_labels)\n","dataset_aug = PascalVOC_Dataset(aug_dir,\n","                                  image_set='train', \n","                                  download=False, \n","                                  transform=transformations, \n","                                  target_transform=encode_labels)\n","\n","dataset_combined = ConcatDataset([dataset_train, dataset_aug])\n","\n","train_loader = DataLoader(dataset_combined, batch_size=batch_size, num_workers=2, shuffle=True)\n","\n","dataset_valid = PascalVOC_Dataset(data_dir, \n","                                  image_set='val', \n","                                  download=False, \n","                                  transform=transformations_valid, \n","                                  target_transform=encode_labels)\n","valid_loader = DataLoader(dataset_valid, batch_size=batch_size, num_workers=2)"],"metadata":{"id":"UGc07TYgrb7L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = resnet18(pretrained=True)\n","net.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n","num_ftrs = net.fc.in_features\n","net.fc = torch.nn.Linear(num_ftrs, num_classes)\n","net = net.to(device)"],"metadata":{"id":"dLoCgbuhreOo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = optim.SGD([{'params': list(net.parameters())[:-1], 'lr': resnet_lr, 'momentum': 0.9},\n","                       {'params': list(net.parameters())[-1], 'lr': fc_lr, 'momentum': 0.9}])\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 12, eta_min=0, last_epoch=-1)\n","criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')"],"metadata":{"id":"JZQy28lc0mGI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_test(net, test_loader, criterion):\n","    correct = 0\n","    total = 0\n","    avg_test_loss = 0.0\n","    l = len(test_loader)\n","    with torch.no_grad():\n","        for _, (images, labels) in enumerate(test_loader):\n","            images, labels = images.to(device), labels.to(device)\n","            \n","            outputs = net(images)\n","            predictions = torch.argmax(outputs, dim=1)\n","            labels = torch.argmax(labels, dim=1)\n","            correct += torch.sum(predictions == labels)\n","            total += labels.size(0)\n","\n","    print(f'Accuracy of the network on the test images: {100 * correct / total:.2f} %')"],"metadata":{"id":"BD3ATtUa0nZX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(net, criterion, optimizer, num_epochs, print_freq = 100):\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        running_correct = 0.0\n","        running_total = 0.0\n","        start_time = time.time()\n","\n","        net.train()\n","\n","        for i, (images, labels) in enumerate(train_loader, 0):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = net(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","            # Get predicted results\n","            predicted = torch.argmax(outputs, dim=1)\n","            labels = torch.argmax(labels, dim=1)\n","\n","            # print statistics\n","            running_loss += loss.item()\n","\n","            # calculate accuracy\n","            running_total += labels.size(0)\n","            running_correct += (predicted == labels).sum().item()\n","\n","            # print every 2000 mini-batches\n","            if i % print_freq == (print_freq - 1):\n","                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / print_freq:.3f} acc: {100*running_correct / running_total:.2f} time: {time.time() - start_time:.2f}')\n","                running_loss, running_correct, running_total = 0.0, 0.0, 0.0\n","                start_time = time.time()\n","\n","        # Run the run_test() function after each epoch\n","        net.eval()\n","        run_test(net, valid_loader, criterion)"],"metadata":{"id":"G5XQUeeQ0qcN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(net, criterion, optimizer, num_epochs=num_epochs)\n","\n","save_dir = os.path.join('/content/drive/MyDrive/DLCV/project/da-fusion/', 'da-fusion.pt')\n","torch.save(net.state_dict(), save_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9dQqzprg0sHV","executionInfo":{"status":"ok","timestamp":1684016898677,"user_tz":300,"elapsed":4886969,"user":{"displayName":"Shrey Sarswat","userId":"00735471870348438365"}},"outputId":"885d558a-3ee5-4d52-9f7a-9f2f85931d8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1,   100] loss: 131.577 acc: 20.66 time: 37.88\n","[1,   200] loss: 111.413 acc: 37.88 time: 37.85\n","Accuracy of the network on the test images: 55.11 %\n","[2,   100] loss: 97.998 acc: 49.03 time: 38.26\n","[2,   200] loss: 89.574 acc: 55.75 time: 37.68\n","Accuracy of the network on the test images: 60.14 %\n","[3,   100] loss: 80.951 acc: 60.50 time: 38.49\n","[3,   200] loss: 77.501 acc: 61.66 time: 37.64\n","Accuracy of the network on the test images: 66.03 %\n","[4,   100] loss: 72.008 acc: 65.97 time: 36.87\n","[4,   200] loss: 69.068 acc: 65.34 time: 37.92\n","Accuracy of the network on the test images: 66.68 %\n","[5,   100] loss: 66.385 acc: 68.12 time: 38.97\n","[5,   200] loss: 64.427 acc: 67.25 time: 38.31\n","Accuracy of the network on the test images: 68.61 %\n","[6,   100] loss: 61.172 acc: 68.38 time: 37.95\n","[6,   200] loss: 60.174 acc: 69.47 time: 37.56\n","Accuracy of the network on the test images: 69.26 %\n","[7,   100] loss: 57.127 acc: 70.59 time: 37.49\n","[7,   200] loss: 56.402 acc: 70.97 time: 38.25\n","Accuracy of the network on the test images: 69.48 %\n","[8,   100] loss: 54.405 acc: 71.62 time: 36.73\n","[8,   200] loss: 54.122 acc: 71.41 time: 37.53\n","Accuracy of the network on the test images: 70.81 %\n","[9,   100] loss: 50.921 acc: 72.56 time: 37.43\n","[9,   200] loss: 51.845 acc: 72.94 time: 37.87\n","Accuracy of the network on the test images: 70.48 %\n","[10,   100] loss: 50.473 acc: 72.94 time: 36.96\n","[10,   200] loss: 48.111 acc: 73.50 time: 38.13\n","Accuracy of the network on the test images: 71.20 %\n","[11,   100] loss: 48.217 acc: 73.59 time: 37.14\n","[11,   200] loss: 45.989 acc: 74.72 time: 39.13\n","Accuracy of the network on the test images: 71.44 %\n","[12,   100] loss: 44.072 acc: 75.03 time: 37.26\n","[12,   200] loss: 45.261 acc: 74.59 time: 37.14\n","Accuracy of the network on the test images: 72.01 %\n","[13,   100] loss: 43.892 acc: 74.19 time: 36.74\n","[13,   200] loss: 42.447 acc: 75.06 time: 37.87\n","Accuracy of the network on the test images: 71.73 %\n","[14,   100] loss: 41.209 acc: 75.94 time: 36.71\n","[14,   200] loss: 40.788 acc: 75.78 time: 37.79\n","Accuracy of the network on the test images: 72.27 %\n","[15,   100] loss: 39.331 acc: 76.53 time: 37.04\n","[15,   200] loss: 39.346 acc: 76.25 time: 37.22\n","Accuracy of the network on the test images: 72.09 %\n","[16,   100] loss: 37.589 acc: 76.59 time: 36.94\n","[16,   200] loss: 37.112 acc: 76.97 time: 37.89\n","Accuracy of the network on the test images: 71.99 %\n","[17,   100] loss: 36.496 acc: 76.91 time: 37.34\n","[17,   200] loss: 36.410 acc: 77.56 time: 37.15\n","Accuracy of the network on the test images: 72.33 %\n","[18,   100] loss: 34.769 acc: 77.00 time: 38.46\n","[18,   200] loss: 34.988 acc: 77.22 time: 37.56\n","Accuracy of the network on the test images: 72.18 %\n","[19,   100] loss: 33.131 acc: 77.72 time: 38.30\n","[19,   200] loss: 33.358 acc: 77.38 time: 38.05\n","Accuracy of the network on the test images: 72.21 %\n","[20,   100] loss: 32.602 acc: 77.53 time: 37.77\n","[20,   200] loss: 31.356 acc: 78.28 time: 37.82\n","Accuracy of the network on the test images: 72.99 %\n","[21,   100] loss: 30.183 acc: 77.72 time: 38.42\n","[21,   200] loss: 31.036 acc: 78.56 time: 37.43\n","Accuracy of the network on the test images: 72.95 %\n","[22,   100] loss: 29.061 acc: 79.47 time: 37.99\n","[22,   200] loss: 29.239 acc: 77.66 time: 36.24\n","Accuracy of the network on the test images: 72.51 %\n","[23,   100] loss: 27.506 acc: 78.66 time: 38.28\n","[23,   200] loss: 28.335 acc: 78.44 time: 36.25\n","Accuracy of the network on the test images: 72.68 %\n","[24,   100] loss: 26.648 acc: 79.19 time: 37.75\n","[24,   200] loss: 26.190 acc: 78.91 time: 36.15\n","Accuracy of the network on the test images: 72.73 %\n","[25,   100] loss: 25.533 acc: 79.84 time: 38.00\n","[25,   200] loss: 24.491 acc: 80.12 time: 35.88\n","Accuracy of the network on the test images: 73.47 %\n","[26,   100] loss: 24.035 acc: 79.84 time: 37.00\n","[26,   200] loss: 23.376 acc: 79.25 time: 36.21\n","Accuracy of the network on the test images: 72.83 %\n","[27,   100] loss: 22.164 acc: 80.53 time: 36.05\n","[27,   200] loss: 22.974 acc: 79.72 time: 37.02\n","Accuracy of the network on the test images: 73.36 %\n","[28,   100] loss: 21.144 acc: 80.59 time: 36.67\n","[28,   200] loss: 21.413 acc: 79.50 time: 36.91\n","Accuracy of the network on the test images: 72.28 %\n","[29,   100] loss: 20.765 acc: 79.28 time: 36.65\n","[29,   200] loss: 20.350 acc: 80.19 time: 37.27\n","Accuracy of the network on the test images: 72.69 %\n","[30,   100] loss: 19.435 acc: 80.44 time: 38.27\n","[30,   200] loss: 19.538 acc: 80.75 time: 37.35\n","Accuracy of the network on the test images: 72.68 %\n","[31,   100] loss: 18.403 acc: 80.56 time: 38.27\n","[31,   200] loss: 18.490 acc: 80.41 time: 37.47\n","Accuracy of the network on the test images: 72.76 %\n","[32,   100] loss: 17.642 acc: 79.91 time: 38.07\n","[32,   200] loss: 17.130 acc: 81.22 time: 36.37\n","Accuracy of the network on the test images: 73.07 %\n","[33,   100] loss: 16.710 acc: 79.31 time: 37.83\n","[33,   200] loss: 16.140 acc: 81.12 time: 36.04\n","Accuracy of the network on the test images: 72.81 %\n","[34,   100] loss: 15.296 acc: 80.50 time: 37.29\n","[34,   200] loss: 16.292 acc: 80.38 time: 36.35\n","Accuracy of the network on the test images: 73.02 %\n","[35,   100] loss: 14.569 acc: 80.78 time: 36.11\n","[35,   200] loss: 14.379 acc: 80.94 time: 37.17\n","Accuracy of the network on the test images: 73.55 %\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"mF1JO-uu00jT"},"execution_count":null,"outputs":[]}]}