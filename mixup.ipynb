{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-YPt63OC7mRq","executionInfo":{"status":"ok","timestamp":1682826684064,"user_tz":300,"elapsed":19998,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}},"outputId":"4d71a27a-ddc0-46c2-b925-6795e9377fbe"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"vRdwFtuzXtgl","executionInfo":{"status":"ok","timestamp":1682826688054,"user_tz":300,"elapsed":3994,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}}},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","import time\n","import tqdm\n","from typing import Tuple\n","import torch.nn as nn\n","import torchvision.models as  models\n","from torch.utils.data import DataLoader\n","import torchvision.datasets.voc as voc\n","import torch.optim as optim\n","from torchvision import transforms\n","from torchvision.models import resnet18\n","import torch.utils.model_zoo as model_zoo"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1682826688055,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"},"user_tz":300},"id":"aQ22btbUZu8E","outputId":"a0134673-fd79-44d4-e25b-bcd21d82841b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fbd304903b0>"]},"metadata":{},"execution_count":3}],"source":["data_dir = '/content/drive/MyDrive/CS 444: DL for CV/Project/data/'\n","ckpt_dir = '/content/drive/MyDrive/CS 444: DL for CV/Project/checkpoints/'\n","object_categories = ['aeroplane', 'bicycle', 'bird', 'boat',\n","                     'bottle', 'bus', 'car', 'cat', 'chair',\n","                     'cow', 'diningtable', 'dog', 'horse',\n","                     'motorbike', 'person', 'pottedplant',\n","                     'sheep', 'sofa', 'train', 'tvmonitor']\n","num_classes = len(object_categories)\n","batch_size = 32\n","resnet_lr = 1e-5\n","fc_lr = 5e-3\n","num_epochs = 25\n","\n","mean = [0.457342265910642, 0.4387686270106377, 0.4073427106250871]\n","std = [0.26753769276329037, 0.2638145880487105, 0.2776826934044154]\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","np.random.seed(1902)\n","torch.manual_seed(1902)"]},{"cell_type":"markdown","metadata":{"id":"wJ-uygocbOw8"},"source":["## Data Pipeline\n","\n","Download the PASCAL VOC dataset and create train and val data loaders."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"bnu_0d_VY3_5","executionInfo":{"status":"ok","timestamp":1682826688055,"user_tz":300,"elapsed":4,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}}},"outputs":[],"source":["class PascalVOC_Dataset(voc.VOCDetection):\n","    \"\"\"Pascal VOC Detection Dataset\"\"\"\n","    def __init__(self, root, image_set='train', download=False, transform=None, target_transform=None):\n","        super().__init__(root, image_set=image_set, download=download, transform=transform, target_transform=target_transform)\n","    \n","    def __getitem__(self, index):\n","        return super().__getitem__(index)\n","    \n","    def __len__(self):\n","        return len(self.images)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"6qNEJbp6aCjE","executionInfo":{"status":"ok","timestamp":1682826688055,"user_tz":300,"elapsed":4,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}}},"outputs":[],"source":["def encode_labels(target):\n","    \"\"\"Encode multiple labels using 1/0 encoding\"\"\"\n","    ls = target['annotation']['object']\n","    j = []\n","    if type(ls) == dict:\n","        if int(ls['difficult']) == 0:\n","            j.append(object_categories.index(ls['name']))\n","    else:\n","        for i in range(len(ls)):\n","            if int(ls[i]['difficult']) == 0:\n","                j.append(object_categories.index(ls[i]['name']))\n","    k = np.zeros(len(object_categories))\n","    k[j] = 1\n","    return torch.from_numpy(k)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"2MycNoV8cwq_","executionInfo":{"status":"ok","timestamp":1682826688056,"user_tz":300,"elapsed":5,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}}},"outputs":[],"source":["transformations = transforms.Compose([transforms.Resize((300, 300)),\n","                                      transforms.ToTensor()])\n","transformations_valid = transforms.Compose([transforms.Resize(330), \n","                                            transforms.CenterCrop(300), \n","                                            transforms.ToTensor()])"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"qR20ICwYZfib","executionInfo":{"status":"ok","timestamp":1682826691200,"user_tz":300,"elapsed":3150,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}}},"outputs":[],"source":["dataset_train = PascalVOC_Dataset(data_dir,\n","                                  image_set='train', \n","                                  download=False, \n","                                  transform=transformations, \n","                                  target_transform=encode_labels)\n","train_loader = DataLoader(dataset_train, batch_size=batch_size, num_workers=2, shuffle=True)\n","\n","dataset_valid = PascalVOC_Dataset(data_dir, \n","                                  image_set='val', \n","                                  download=False, \n","                                  transform=transformations_valid, \n","                                  target_transform=encode_labels)\n","valid_loader = DataLoader(dataset_valid, batch_size=batch_size, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"1NNfmBcIbaif"},"source":["## Define Model"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ZIcFhPiqbOBR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682826699588,"user_tz":300,"elapsed":6278,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}},"outputId":"2a696300-9069-49c8-ac49-b7201ac902ec"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:01<00:00, 42.0MB/s]\n"]}],"source":["net = resnet18(pretrained=True)\n","net.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n","num_ftrs = net.fc.in_features\n","net.fc = torch.nn.Linear(num_ftrs, num_classes)\n","net = net.to(device)"]},{"cell_type":"markdown","metadata":{"id":"hSuc51ptcBFZ"},"source":["## Define Training Parameters"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"d0qXwNxrcDNa","executionInfo":{"status":"ok","timestamp":1682826700631,"user_tz":300,"elapsed":262,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}}},"outputs":[],"source":["optimizer = optim.SGD([{'params': list(net.parameters())[:-1], 'lr': resnet_lr, 'momentum': 0.9},\n","                       {'params': list(net.parameters())[-1], 'lr': fc_lr, 'momentum': 0.9}])\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 12, eta_min=0, last_epoch=-1)\n","criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')"]},{"cell_type":"code","source":["class RandomMixup(torch.nn.Module):\n","    \"\"\"Randomly apply Mixup to the provided batch and targets.\n","    The class implements the data augmentations as described in the paper\n","    `\"mixup: Beyond Empirical Risk Minimization\" <https://arxiv.org/abs/1710.09412>`_.\n","    Args:\n","        num_classes (int): number of classes used for one-hot encoding.\n","        p (float): probability of the batch being transformed. Default value is 0.5.\n","        alpha (float): hyperparameter of the Beta distribution used for mixup.\n","            Default value is 1.0.\n","        inplace (bool): boolean to make this transform inplace. Default set to False.\n","    \"\"\"\n","\n","    def __init__(self, num_classes: int, p: float = 0.5, alpha: float = 1.0, inplace: bool = False) -> None:\n","        super().__init__()\n","\n","        if num_classes < 1:\n","            raise ValueError(\n","                f\"Please provide a valid positive value for the num_classes. Got num_classes={num_classes}\"\n","            )\n","\n","        if alpha <= 0:\n","            raise ValueError(\"Alpha param can't be zero.\")\n","\n","        self.num_classes = num_classes\n","        self.p = p\n","        self.alpha = alpha\n","        self.inplace = inplace\n","\n","    def forward(self, batch: torch.Tensor, target: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        Args:\n","            batch (Tensor): Float tensor of size (B, C, H, W)\n","            target (Tensor): Integer tensor of size (B, )\n","        Returns:\n","            Tensor: Randomly transformed batch.\n","        \"\"\"\n","        if batch.ndim != 4:\n","            raise ValueError(f\"Batch ndim should be 4. Got {batch.ndim}\")\n","        if target.ndim != 1:\n","            raise ValueError(f\"Target ndim should be 1. Got {target.ndim}\")\n","        if not batch.is_floating_point():\n","            raise TypeError(f\"Batch dtype should be a float tensor. Got {batch.dtype}.\")\n","        if target.dtype != torch.int64:\n","            raise TypeError(f\"Target dtype should be torch.int64. Got {target.dtype}\")\n","\n","        if not self.inplace:\n","            batch = batch.clone()\n","            target = target.clone()\n","\n","        if target.ndim == 1:\n","            target = torch.nn.functional.one_hot(target, num_classes=self.num_classes).to(dtype=batch.dtype)\n","\n","        if torch.rand(1).item() >= self.p:\n","            return batch, target\n","\n","        batch_rolled = batch.roll(1, 0)\n","        target_rolled = target.roll(1, 0)\n","\n","        lambda_param = float(torch._sample_dirichlet(torch.tensor([self.alpha, self.alpha]))[0])\n","        batch_rolled.mul_(1.0 - lambda_param)\n","        batch.mul_(lambda_param).add_(batch_rolled)\n","\n","        target_rolled.mul_(1.0 - lambda_param)\n","        target.mul_(lambda_param).add_(target_rolled)\n","\n","        return batch, target\n","\n","    def __repr__(self) -> str:\n","        s = (\n","            f\"{self.__class__.__name__}(\"\n","            f\"num_classes={self.num_classes}\"\n","            f\", p={self.p}\"\n","            f\", alpha={self.alpha}\"\n","            f\", inplace={self.inplace}\"\n","            f\")\"\n","        )\n","        return s"],"metadata":{"id":"k1tgDvMcFbDH","executionInfo":{"status":"ok","timestamp":1682826702923,"user_tz":300,"elapsed":195,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Tru2BAex5DNe","executionInfo":{"status":"ok","timestamp":1682826725275,"user_tz":300,"elapsed":283,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}}},"outputs":[],"source":["def run_test(net, test_loader, criterion):\n","    correct = 0\n","    total = 0\n","    avg_test_loss = 0.0\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            \n","            outputs = net(images)\n","            predictions = torch.argmax(outputs, dim=1)\n","            labels = torch.argmax(labels, dim=1)\n","            correct += torch.sum(predictions == labels)\n","            total += labels.size(0)\n","\n","    print(f'Accuracy of the network on the test images: {100 * correct / total:.2f} %')"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"eZGfBjDW4b9r","executionInfo":{"status":"ok","timestamp":1682826727902,"user_tz":300,"elapsed":247,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}}},"outputs":[],"source":["def train(net, criterion, optimizer, num_epochs, print_freq = 100):\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        running_correct = 0.0\n","        running_total = 0.0\n","        start_time = time.time()\n","\n","        net.train()\n","\n","        for i, (images, labels) in enumerate(train_loader, 0):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            images, labels = RandomMixup(num_classes, p=1, alpha=0.5)(images, torch.argmax(labels, dim=1))\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = net(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","            # Get predicted results\n","            predicted = torch.argmax(outputs, dim=1)\n","            labels = torch.argmax(labels, dim=1)\n","\n","            # print statistics\n","            running_loss += loss.item()\n","\n","            # calculate accuracy\n","            running_total += labels.size(0)\n","            running_correct += (predicted == labels).sum().item()\n","\n","            # print every 2000 mini-batches\n","            if i % print_freq == (print_freq - 1):\n","                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / print_freq:.3f} acc: {100*running_correct / running_total:.2f} time: {time.time() - start_time:.2f}')\n","                running_loss, running_correct, running_total = 0.0, 0.0, 0.0\n","                start_time = time.time()\n","\n","        # Run the run_test() function after each epoch\n","        net.eval()\n","        run_test(net, valid_loader, criterion)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"f8gpDFo_BCNN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682834062958,"user_tz":300,"elapsed":7327722,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}},"outputId":"92303601-bd9f-496e-9a1d-f34fbcbd1873"},"outputs":[{"output_type":"stream","name":"stdout","text":["[1,   100] loss: 137.396 acc: 14.81 time: 1275.59\n","Accuracy of the network on the test images: 32.20 %\n","[2,   100] loss: 105.691 acc: 35.44 time: 38.04\n","Accuracy of the network on the test images: 52.84 %\n","[3,   100] loss: 95.932 acc: 49.22 time: 38.28\n","Accuracy of the network on the test images: 59.90 %\n","[4,   100] loss: 87.598 acc: 55.44 time: 36.65\n","Accuracy of the network on the test images: 65.64 %\n","[5,   100] loss: 80.450 acc: 62.53 time: 39.13\n","Accuracy of the network on the test images: 68.49 %\n","[6,   100] loss: 79.759 acc: 61.81 time: 38.14\n","Accuracy of the network on the test images: 69.60 %\n","[7,   100] loss: 79.202 acc: 64.12 time: 38.28\n","Accuracy of the network on the test images: 71.63 %\n","[8,   100] loss: 75.742 acc: 66.16 time: 38.32\n","Accuracy of the network on the test images: 72.27 %\n","[9,   100] loss: 73.438 acc: 67.50 time: 36.51\n","Accuracy of the network on the test images: 72.92 %\n","[10,   100] loss: 72.040 acc: 67.81 time: 39.84\n","Accuracy of the network on the test images: 71.66 %\n","[11,   100] loss: 74.123 acc: 67.66 time: 37.91\n","Accuracy of the network on the test images: 74.17 %\n","[12,   100] loss: 73.829 acc: 67.47 time: 36.57\n","Accuracy of the network on the test images: 74.55 %\n","[13,   100] loss: 71.747 acc: 69.72 time: 37.64\n","Accuracy of the network on the test images: 74.72 %\n","[14,   100] loss: 67.587 acc: 70.34 time: 39.44\n","Accuracy of the network on the test images: 74.50 %\n","[15,   100] loss: 73.225 acc: 67.91 time: 37.87\n","Accuracy of the network on the test images: 75.30 %\n","[16,   100] loss: 67.108 acc: 72.75 time: 36.88\n","Accuracy of the network on the test images: 75.65 %\n","[17,   100] loss: 67.757 acc: 72.06 time: 37.30\n","Accuracy of the network on the test images: 75.36 %\n","[18,   100] loss: 68.973 acc: 71.22 time: 37.92\n","Accuracy of the network on the test images: 76.08 %\n","[19,   100] loss: 67.376 acc: 72.97 time: 38.16\n","Accuracy of the network on the test images: 76.46 %\n","[20,   100] loss: 66.274 acc: 73.28 time: 36.56\n","Accuracy of the network on the test images: 76.59 %\n","[21,   100] loss: 62.610 acc: 74.59 time: 37.03\n","Accuracy of the network on the test images: 76.34 %\n","[22,   100] loss: 66.247 acc: 73.56 time: 38.15\n","Accuracy of the network on the test images: 77.16 %\n","[23,   100] loss: 64.537 acc: 74.41 time: 40.14\n","Accuracy of the network on the test images: 77.47 %\n","[24,   100] loss: 61.434 acc: 76.53 time: 38.54\n","Accuracy of the network on the test images: 77.40 %\n","[25,   100] loss: 63.913 acc: 75.25 time: 36.83\n","Accuracy of the network on the test images: 77.26 %\n"]}],"source":["train(net, criterion, optimizer, num_epochs=num_epochs)\n","\n","save_dir = os.path.join(ckpt_dir, 'mixup.pt')\n","torch.save(net.state_dict(), save_dir)"]},{"cell_type":"code","source":[],"metadata":{"id":"1bhpJv2SiOfC"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1tnNexWUhj0CnOK410etU-i1g_ENvr5kn","timestamp":1681615045876}],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}