{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-YPt63OC7mRq","executionInfo":{"status":"ok","timestamp":1683785328615,"user_tz":300,"elapsed":27927,"user":{"displayName":"Savya Khosla","userId":"04060910677193617509"}},"outputId":"6a53cbc2-7da0-44fb-a7bf-28791f744c42"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"vRdwFtuzXtgl","executionInfo":{"status":"ok","timestamp":1683785334694,"user_tz":300,"elapsed":6081,"user":{"displayName":"Savya Khosla","userId":"04060910677193617509"}}},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","import math\n","import time\n","import tqdm\n","from typing import Tuple\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.models as  models\n","from torch.utils.data import DataLoader\n","import torchvision.datasets.voc as voc\n","import torch.optim as optim\n","from torchvision import transforms\n","from torchvision.models import resnet18\n","import torch.utils.model_zoo as model_zoo"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1683785334694,"user":{"displayName":"Savya Khosla","userId":"04060910677193617509"},"user_tz":300},"id":"aQ22btbUZu8E","outputId":"58548993-b075-483a-b1db-611046c5681d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f774803a310>"]},"metadata":{},"execution_count":3}],"source":["data_dir = '/content/drive/MyDrive/CS 444: DL for CV/Project/data/'\n","ckpt_dir = '/content/drive/MyDrive/CS 444: DL for CV/Project/checkpoints/'\n","object_categories = ['aeroplane', 'bicycle', 'bird', 'boat',\n","                     'bottle', 'bus', 'car', 'cat', 'chair',\n","                     'cow', 'diningtable', 'dog', 'horse',\n","                     'motorbike', 'person', 'pottedplant',\n","                     'sheep', 'sofa', 'train', 'tvmonitor']\n","num_classes = len(object_categories)\n","batch_size = 32\n","resnet_lr = 1e-5\n","fc_lr = 5e-3\n","num_epochs = 25\n","\n","mean = [0.457342265910642, 0.4387686270106377, 0.4073427106250871]\n","std = [0.26753769276329037, 0.2638145880487105, 0.2776826934044154]\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","np.random.seed(1902)\n","torch.manual_seed(1902)"]},{"cell_type":"markdown","metadata":{"id":"wJ-uygocbOw8"},"source":["## Data Pipeline\n","\n","Download the PASCAL VOC dataset and create train and val data loaders."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"bnu_0d_VY3_5","executionInfo":{"status":"ok","timestamp":1683785334694,"user_tz":300,"elapsed":2,"user":{"displayName":"Savya Khosla","userId":"04060910677193617509"}}},"outputs":[],"source":["class PascalVOC_Dataset(voc.VOCDetection):\n","    \"\"\"Pascal VOC Detection Dataset\"\"\"\n","    def __init__(self, root, image_set='train', download=False, transform=None, target_transform=None):\n","        super().__init__(root, image_set=image_set, download=download, transform=transform, target_transform=target_transform)\n","    \n","    def __getitem__(self, index):\n","        return super().__getitem__(index)\n","    \n","    def __len__(self):\n","        return len(self.images)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"6qNEJbp6aCjE","executionInfo":{"status":"ok","timestamp":1683785334694,"user_tz":300,"elapsed":2,"user":{"displayName":"Savya Khosla","userId":"04060910677193617509"}}},"outputs":[],"source":["def encode_labels(target):\n","    \"\"\"Encode multiple labels using 1/0 encoding\"\"\"\n","    ls = target['annotation']['object']\n","    j = []\n","    if type(ls) == dict:\n","        if int(ls['difficult']) == 0:\n","            j.append(object_categories.index(ls['name']))\n","    else:\n","        for i in range(len(ls)):\n","            if int(ls[i]['difficult']) == 0:\n","                j.append(object_categories.index(ls[i]['name']))\n","    k = np.zeros(len(object_categories))\n","    k[j] = 1\n","    return torch.from_numpy(k)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"2MycNoV8cwq_","executionInfo":{"status":"ok","timestamp":1683785334694,"user_tz":300,"elapsed":2,"user":{"displayName":"Savya Khosla","userId":"04060910677193617509"}}},"outputs":[],"source":["transformations = transforms.Compose([transforms.Resize((300, 300)),\n","                                      transforms.ToTensor()])\n","transformations_valid = transforms.Compose([transforms.Resize(330), \n","                                            transforms.CenterCrop(300), \n","                                            transforms.ToTensor()])"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"qR20ICwYZfib","executionInfo":{"status":"ok","timestamp":1683785340220,"user_tz":300,"elapsed":5528,"user":{"displayName":"Savya Khosla","userId":"04060910677193617509"}}},"outputs":[],"source":["dataset_train = PascalVOC_Dataset(data_dir,\n","                                  image_set='train', \n","                                  download=False, \n","                                  transform=transformations, \n","                                  target_transform=encode_labels)\n","train_loader = DataLoader(dataset_train, batch_size=batch_size, num_workers=2, shuffle=True)\n","\n","dataset_valid = PascalVOC_Dataset(data_dir, \n","                                  image_set='val', \n","                                  download=False, \n","                                  transform=transformations_valid, \n","                                  target_transform=encode_labels)\n","valid_loader = DataLoader(dataset_valid, batch_size=batch_size, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"1NNfmBcIbaif"},"source":["## Define Model"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ZIcFhPiqbOBR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683785346464,"user_tz":300,"elapsed":6249,"user":{"displayName":"Savya Khosla","userId":"04060910677193617509"}},"outputId":"c5063060-ebd6-4dca-8b1d-1111a1fa2627"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 196MB/s]\n"]}],"source":["net = resnet18(pretrained=True)\n","net.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n","num_ftrs = net.fc.in_features\n","net.fc = torch.nn.Linear(num_ftrs, num_classes)\n","net = net.to(device)"]},{"cell_type":"markdown","metadata":{"id":"hSuc51ptcBFZ"},"source":["## Define Training Parameters"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"d0qXwNxrcDNa","executionInfo":{"status":"ok","timestamp":1683785346464,"user_tz":300,"elapsed":4,"user":{"displayName":"Savya Khosla","userId":"04060910677193617509"}}},"outputs":[],"source":["optimizer = optim.SGD([{'params': list(net.parameters())[:-1], 'lr': resnet_lr, 'momentum': 0.9},\n","                       {'params': list(net.parameters())[-1], 'lr': fc_lr, 'momentum': 0.9}])\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 12, eta_min=0, last_epoch=-1)\n","criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')"]},{"cell_type":"code","source":["class Cutout(object):\n","    \"\"\"Randomly mask out one or more patches from an image.\n","    Args:\n","        n_holes (int): Number of patches to cut out of each image.\n","        length (int): The length (in pixels) of each square patch.\n","    \"\"\"\n","    def __init__(self, n_holes, length):\n","        self.n_holes = n_holes\n","        self.length = length\n","\n","    def __call__(self, images):\n","        \"\"\"\n","        Args:\n","            images (Tensor): Batch of images of size (B, C, H, W).\n","        Returns:\n","            Tensor: Images with n_holes of dimension length x length cut out of it.\n","        \"\"\"\n","        for i in range(images.shape[0]):\n","            img = images[i]\n","            h = img.size(1)\n","            w = img.size(2)\n","            mask = np.ones((h, w), np.float32)\n","\n","            for n in range(self.n_holes):\n","                y = np.random.randint(h)\n","                x = np.random.randint(w)\n","\n","                len = np.random.randint(self.length[0], self.length[1])\n","                y1 = np.clip(y - len // 2, 0, h)\n","                y2 = np.clip(y + len // 2, 0, h)\n","                x1 = np.clip(x - len // 2, 0, w)\n","                x2 = np.clip(x + len // 2, 0, w)\n","\n","                mask[y1: y2, x1: x2] = 0.\n","\n","            mask = torch.from_numpy(mask).to(device)\n","            mask = mask.expand_as(img)\n","            img = img * mask\n","            images[i] = img\n","\n","        return images"],"metadata":{"id":"k1tgDvMcFbDH","executionInfo":{"status":"ok","timestamp":1683785346465,"user_tz":300,"elapsed":4,"user":{"displayName":"Savya Khosla","userId":"04060910677193617509"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Tru2BAex5DNe","executionInfo":{"status":"ok","timestamp":1683785346465,"user_tz":300,"elapsed":4,"user":{"displayName":"Savya Khosla","userId":"04060910677193617509"}}},"outputs":[],"source":["def run_test(net, test_loader, criterion):\n","    correct = 0\n","    total = 0\n","    avg_test_loss = 0.0\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            \n","            outputs = net(images)\n","            predictions = torch.argmax(outputs, dim=1)\n","            labels = torch.argmax(labels, dim=1)\n","            correct += torch.sum(predictions == labels)\n","            total += labels.size(0)\n","\n","    print(f'Accuracy of the network on the test images: {100 * correct / total:.2f} %')"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"eZGfBjDW4b9r","executionInfo":{"status":"ok","timestamp":1683785346465,"user_tz":300,"elapsed":3,"user":{"displayName":"Savya Khosla","userId":"04060910677193617509"}}},"outputs":[],"source":["def train(net, criterion, optimizer, num_epochs, print_freq = 100):\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        running_correct = 0.0\n","        running_total = 0.0\n","        start_time = time.time()\n","\n","        net.train()\n","\n","        for i, (images, labels) in enumerate(train_loader, 0):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            images = Cutout(n_holes=1, length=[50, 150])(images)\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = net(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","            # Get predicted results\n","            predicted = torch.argmax(outputs, dim=1)\n","            labels = torch.argmax(labels, dim=1)\n","\n","            # print statistics\n","            running_loss += loss.item()\n","\n","            # calculate accuracy\n","            running_total += labels.size(0)\n","            running_correct += (predicted == labels).sum().item()\n","\n","            # print every 2000 mini-batches\n","            if i % print_freq == (print_freq - 1):\n","                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / print_freq:.3f} acc: {100*running_correct / running_total:.2f} time: {time.time() - start_time:.2f}')\n","                running_loss, running_correct, running_total = 0.0, 0.0, 0.0\n","                start_time = time.time()\n","\n","        # Run the run_test() function after each epoch\n","        net.eval()\n","        run_test(net, valid_loader, criterion)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"f8gpDFo_BCNN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ab7a6e92-70b9-4edf-9ba3-c9cf7c6b63bc","executionInfo":{"status":"ok","timestamp":1683794298010,"user_tz":300,"elapsed":8939967,"user":{"displayName":"Savya Khosla","userId":"04060910677193617509"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[1,   100] loss: 164.190 acc: 13.25 time: 1704.62\n","Accuracy of the network on the test images: 29.68 %\n","[2,   100] loss: 117.138 acc: 34.09 time: 38.20\n","Accuracy of the network on the test images: 51.61 %\n","[3,   100] loss: 96.738 acc: 49.25 time: 39.73\n","Accuracy of the network on the test images: 60.19 %\n","[4,   100] loss: 85.796 acc: 55.91 time: 43.21\n","Accuracy of the network on the test images: 63.11 %\n","[5,   100] loss: 77.894 acc: 60.56 time: 43.68\n","Accuracy of the network on the test images: 66.27 %\n","[6,   100] loss: 72.532 acc: 63.12 time: 43.75\n","Accuracy of the network on the test images: 66.65 %\n","[7,   100] loss: 69.144 acc: 64.47 time: 43.30\n","Accuracy of the network on the test images: 67.66 %\n","[8,   100] loss: 64.174 acc: 65.66 time: 42.44\n","Accuracy of the network on the test images: 68.18 %\n","[9,   100] loss: 61.402 acc: 67.34 time: 42.10\n","Accuracy of the network on the test images: 69.36 %\n","[10,   100] loss: 60.511 acc: 66.81 time: 42.87\n","Accuracy of the network on the test images: 69.00 %\n","[11,   100] loss: 58.077 acc: 68.12 time: 41.97\n","Accuracy of the network on the test images: 70.22 %\n","[12,   100] loss: 54.904 acc: 68.94 time: 40.23\n","Accuracy of the network on the test images: 70.62 %\n","[13,   100] loss: 55.127 acc: 68.22 time: 40.24\n","Accuracy of the network on the test images: 71.08 %\n","[14,   100] loss: 50.970 acc: 71.06 time: 38.93\n","Accuracy of the network on the test images: 71.18 %\n","[15,   100] loss: 50.652 acc: 70.03 time: 40.22\n","Accuracy of the network on the test images: 71.37 %\n","[16,   100] loss: 48.816 acc: 70.25 time: 39.66\n","Accuracy of the network on the test images: 71.54 %\n","[17,   100] loss: 46.656 acc: 71.38 time: 38.83\n","Accuracy of the network on the test images: 71.65 %\n","[18,   100] loss: 46.115 acc: 71.38 time: 39.73\n","Accuracy of the network on the test images: 71.48 %\n","[19,   100] loss: 44.055 acc: 72.72 time: 38.70\n","Accuracy of the network on the test images: 71.58 %\n","[20,   100] loss: 42.955 acc: 72.38 time: 40.15\n","Accuracy of the network on the test images: 71.94 %\n","[21,   100] loss: 41.598 acc: 72.94 time: 40.33\n","Accuracy of the network on the test images: 72.32 %\n","[22,   100] loss: 41.623 acc: 72.75 time: 38.59\n","Accuracy of the network on the test images: 72.06 %\n","[23,   100] loss: 39.633 acc: 73.38 time: 39.40\n","Accuracy of the network on the test images: 72.28 %\n","[24,   100] loss: 39.038 acc: 74.00 time: 39.59\n","Accuracy of the network on the test images: 72.08 %\n","[25,   100] loss: 37.603 acc: 74.34 time: 39.40\n","Accuracy of the network on the test images: 72.92 %\n"]}],"source":["train(net, criterion, optimizer, num_epochs=num_epochs)\n","\n","save_dir = os.path.join(ckpt_dir, 'cutout.pt')\n","torch.save(net.state_dict(), save_dir)"]},{"cell_type":"code","source":[],"metadata":{"id":"eon9T4hSqLj_"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"14Q-M_iJR56MlFE57wyspqgpruaHrM738","timestamp":1683183652401},{"file_id":"1NcIsYDOhdXf1BGdVQTL3oP0Kqc-9Xxal","timestamp":1681704052349},{"file_id":"1tnNexWUhj0CnOK410etU-i1g_ENvr5kn","timestamp":1681615045876}],"gpuType":"T4"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}