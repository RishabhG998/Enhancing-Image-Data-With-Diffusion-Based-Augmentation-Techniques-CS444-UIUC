{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-YPt63OC7mRq","executionInfo":{"status":"ok","timestamp":1682838340526,"user_tz":300,"elapsed":23358,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}},"outputId":"6e11032a-0f52-4293-95af-cdb43971b267"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":16,"metadata":{"id":"vRdwFtuzXtgl","executionInfo":{"status":"ok","timestamp":1682839169077,"user_tz":300,"elapsed":2,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}}},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","import math\n","import time\n","import tqdm\n","from typing import Tuple\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.models as  models\n","from torch.utils.data import DataLoader\n","import torchvision.datasets.voc as voc\n","import torch.optim as optim\n","from torchvision import transforms\n","from torchvision.models import resnet18\n","import torch.utils.model_zoo as model_zoo"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1682838344796,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"},"user_tz":300},"id":"aQ22btbUZu8E","outputId":"479f06c3-47fa-4b61-b244-5b410359af1b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fd9f821c490>"]},"metadata":{},"execution_count":3}],"source":["data_dir = '/content/drive/MyDrive/CS 444: DL for CV/Project/data/'\n","ckpt_dir = '/content/drive/MyDrive/CS 444: DL for CV/Project/checkpoints/'\n","object_categories = ['aeroplane', 'bicycle', 'bird', 'boat',\n","                     'bottle', 'bus', 'car', 'cat', 'chair',\n","                     'cow', 'diningtable', 'dog', 'horse',\n","                     'motorbike', 'person', 'pottedplant',\n","                     'sheep', 'sofa', 'train', 'tvmonitor']\n","num_classes = len(object_categories)\n","batch_size = 32\n","resnet_lr = 1e-5\n","fc_lr = 5e-3\n","num_epochs = 25\n","\n","mean = [0.457342265910642, 0.4387686270106377, 0.4073427106250871]\n","std = [0.26753769276329037, 0.2638145880487105, 0.2776826934044154]\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","np.random.seed(1902)\n","torch.manual_seed(1902)"]},{"cell_type":"markdown","metadata":{"id":"wJ-uygocbOw8"},"source":["## Data Pipeline\n","\n","Download the PASCAL VOC dataset and create train and val data loaders."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"bnu_0d_VY3_5","executionInfo":{"status":"ok","timestamp":1682838344797,"user_tz":300,"elapsed":5,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}}},"outputs":[],"source":["class PascalVOC_Dataset(voc.VOCDetection):\n","    \"\"\"Pascal VOC Detection Dataset\"\"\"\n","    def __init__(self, root, image_set='train', download=False, transform=None, target_transform=None):\n","        super().__init__(root, image_set=image_set, download=download, transform=transform, target_transform=target_transform)\n","    \n","    def __getitem__(self, index):\n","        return super().__getitem__(index)\n","    \n","    def __len__(self):\n","        return len(self.images)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"6qNEJbp6aCjE","executionInfo":{"status":"ok","timestamp":1682838345023,"user_tz":300,"elapsed":230,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}}},"outputs":[],"source":["def encode_labels(target):\n","    \"\"\"Encode multiple labels using 1/0 encoding\"\"\"\n","    ls = target['annotation']['object']\n","    j = []\n","    if type(ls) == dict:\n","        if int(ls['difficult']) == 0:\n","            j.append(object_categories.index(ls['name']))\n","    else:\n","        for i in range(len(ls)):\n","            if int(ls[i]['difficult']) == 0:\n","                j.append(object_categories.index(ls[i]['name']))\n","    k = np.zeros(len(object_categories))\n","    k[j] = 1\n","    return torch.from_numpy(k)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"2MycNoV8cwq_","executionInfo":{"status":"ok","timestamp":1682838345024,"user_tz":300,"elapsed":2,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}}},"outputs":[],"source":["transformations = transforms.Compose([transforms.Resize((300, 300)),\n","                                      transforms.ToTensor()])\n","transformations_valid = transforms.Compose([transforms.Resize(330), \n","                                            transforms.CenterCrop(300), \n","                                            transforms.ToTensor()])"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"qR20ICwYZfib","executionInfo":{"status":"ok","timestamp":1682838348970,"user_tz":300,"elapsed":3948,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}}},"outputs":[],"source":["dataset_train = PascalVOC_Dataset(data_dir,\n","                                  image_set='train', \n","                                  download=False, \n","                                  transform=transformations, \n","                                  target_transform=encode_labels)\n","train_loader = DataLoader(dataset_train, batch_size=batch_size, num_workers=2, shuffle=True)\n","\n","dataset_valid = PascalVOC_Dataset(data_dir, \n","                                  image_set='val', \n","                                  download=False, \n","                                  transform=transformations_valid, \n","                                  target_transform=encode_labels)\n","valid_loader = DataLoader(dataset_valid, batch_size=batch_size, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"1NNfmBcIbaif"},"source":["## Define Model"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ZIcFhPiqbOBR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682838355081,"user_tz":300,"elapsed":6114,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}},"outputId":"c44a773e-c4ec-4c74-ee4d-6672472d11e1"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 53.6MB/s]\n"]}],"source":["net = resnet18(pretrained=True)\n","net.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n","num_ftrs = net.fc.in_features\n","net.fc = torch.nn.Linear(num_ftrs, num_classes)\n","net = net.to(device)"]},{"cell_type":"markdown","metadata":{"id":"hSuc51ptcBFZ"},"source":["## Define Training Parameters"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"d0qXwNxrcDNa","executionInfo":{"status":"ok","timestamp":1682838355082,"user_tz":300,"elapsed":21,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}}},"outputs":[],"source":["optimizer = optim.SGD([{'params': list(net.parameters())[:-1], 'lr': resnet_lr, 'momentum': 0.9},\n","                       {'params': list(net.parameters())[-1], 'lr': fc_lr, 'momentum': 0.9}])\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 12, eta_min=0, last_epoch=-1)\n","criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')"]},{"cell_type":"code","source":["class RandomCutmix(torch.nn.Module):\n","    \"\"\"Randomly apply Cutmix to the provided batch and targets.\n","    The class implements the data augmentations as described in the paper\n","    `\"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features\"\n","    <https://arxiv.org/abs/1905.04899>`_.\n","    Args:\n","        num_classes (int): number of classes used for one-hot encoding.\n","        p (float): probability of the batch being transformed. Default value is 0.5.\n","        alpha (float): hyperparameter of the Beta distribution used for cutmix.\n","            Default value is 1.0.\n","        inplace (bool): boolean to make this transform inplace. Default set to False.\n","    \"\"\"\n","\n","    def __init__(self, num_classes: int, p: float = 0.5, alpha: float = 1.0, inplace: bool = False) -> None:\n","        super().__init__()\n","        if num_classes < 1:\n","            raise ValueError(\"Please provide a valid positive value for the num_classes.\")\n","        if alpha <= 0:\n","            raise ValueError(\"Alpha param can't be zero.\")\n","\n","        self.num_classes = num_classes\n","        self.p = p\n","        self.alpha = alpha\n","        self.inplace = inplace\n","\n","    def forward(self, batch: torch.Tensor, target: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        Args:\n","            batch (Tensor): Float tensor of size (B, C, H, W)\n","            target (Tensor): Integer tensor of size (B, )\n","        Returns:\n","            Tensor: Randomly transformed batch.\n","        \"\"\"\n","        if batch.ndim != 4:\n","            raise ValueError(f\"Batch ndim should be 4. Got {batch.ndim}\")\n","        if target.ndim != 1:\n","            raise ValueError(f\"Target ndim should be 1. Got {target.ndim}\")\n","        if not batch.is_floating_point():\n","            raise TypeError(f\"Batch dtype should be a float tensor. Got {batch.dtype}.\")\n","        if target.dtype != torch.int64:\n","            raise TypeError(f\"Target dtype should be torch.int64. Got {target.dtype}\")\n","\n","        if not self.inplace:\n","            batch = batch.clone()\n","            target = target.clone()\n","\n","        if target.ndim == 1:\n","            target = torch.nn.functional.one_hot(target, num_classes=self.num_classes).to(dtype=batch.dtype)\n","\n","        if torch.rand(1).item() >= self.p:\n","            return batch, target\n","\n","        # It's faster to roll the batch by one instead of shuffling it to create image pairs\n","        batch_rolled = batch.roll(1, 0)\n","        target_rolled = target.roll(1, 0)\n","\n","        # Implemented as on cutmix paper, page 12 (with minor corrections on typos).\n","        lambda_param = float(torch._sample_dirichlet(torch.tensor([self.alpha, self.alpha]))[0])\n","        _, _, H, W = batch.size()\n","\n","        r_x = torch.randint(W, (1,))\n","        r_y = torch.randint(H, (1,))\n","\n","        r = 0.5 * math.sqrt(1.0 - lambda_param)\n","        r_w_half = int(r * W)\n","        r_h_half = int(r * H)\n","\n","        x1 = int(torch.clamp(r_x - r_w_half, min=0))\n","        y1 = int(torch.clamp(r_y - r_h_half, min=0))\n","        x2 = int(torch.clamp(r_x + r_w_half, max=W))\n","        y2 = int(torch.clamp(r_y + r_h_half, max=H))\n","\n","        batch[:, :, y1:y2, x1:x2] = batch_rolled[:, :, y1:y2, x1:x2]\n","        lambda_param = float(1.0 - (x2 - x1) * (y2 - y1) / (W * H))\n","\n","        target_rolled.mul_(1.0 - lambda_param)\n","        target.mul_(lambda_param).add_(target_rolled)\n","\n","        return batch, target\n","\n","    def __repr__(self) -> str:\n","        s = (\n","            f\"{self.__class__.__name__}(\"\n","            f\"num_classes={self.num_classes}\"\n","            f\", p={self.p}\"\n","            f\", alpha={self.alpha}\"\n","            f\", inplace={self.inplace}\"\n","            f\")\"\n","        )\n","        return s"],"metadata":{"id":"k1tgDvMcFbDH","executionInfo":{"status":"ok","timestamp":1682839367094,"user_tz":300,"elapsed":525,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Tru2BAex5DNe","executionInfo":{"status":"ok","timestamp":1682838355083,"user_tz":300,"elapsed":19,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}}},"outputs":[],"source":["def run_test(net, test_loader, criterion):\n","    correct = 0\n","    total = 0\n","    avg_test_loss = 0.0\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            \n","            outputs = net(images)\n","            predictions = torch.argmax(outputs, dim=1)\n","            labels = torch.argmax(labels, dim=1)\n","            correct += torch.sum(predictions == labels)\n","            total += labels.size(0)\n","\n","    print(f'Accuracy of the network on the test images: {100 * correct / total:.2f} %')"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"eZGfBjDW4b9r","executionInfo":{"status":"ok","timestamp":1682838355083,"user_tz":300,"elapsed":18,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}}},"outputs":[],"source":["def train(net, criterion, optimizer, num_epochs, print_freq = 100):\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        running_correct = 0.0\n","        running_total = 0.0\n","        start_time = time.time()\n","\n","        net.train()\n","\n","        for i, (images, labels) in enumerate(train_loader, 0):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            images, labels = RandomCutmix(num_classes, p=1, alpha=0.5)(images, torch.argmax(labels, dim=1))\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = net(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","            # Get predicted results\n","            predicted = torch.argmax(outputs, dim=1)\n","            labels = torch.argmax(labels, dim=1)\n","\n","            # print statistics\n","            running_loss += loss.item()\n","\n","            # calculate accuracy\n","            running_total += labels.size(0)\n","            running_correct += (predicted == labels).sum().item()\n","\n","            # print every 2000 mini-batches\n","            if i % print_freq == (print_freq - 1):\n","                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / print_freq:.3f} acc: {100*running_correct / running_total:.2f} time: {time.time() - start_time:.2f}')\n","                running_loss, running_correct, running_total = 0.0, 0.0, 0.0\n","                start_time = time.time()\n","\n","        # Run the run_test() function after each epoch\n","        net.eval()\n","        run_test(net, valid_loader, criterion)"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"f8gpDFo_BCNN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682847483312,"user_tz":300,"elapsed":8111833,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}},"outputId":"718c52f4-cd75-4bd2-9a73-788ab74c0018"},"outputs":[{"output_type":"stream","name":"stdout","text":["[1,   100] loss: 137.794 acc: 13.28 time: 1373.78\n","Accuracy of the network on the test images: 27.39 %\n","[2,   100] loss: 108.566 acc: 30.84 time: 35.82\n","Accuracy of the network on the test images: 50.39 %\n","[3,   100] loss: 100.700 acc: 41.03 time: 35.50\n","Accuracy of the network on the test images: 60.42 %\n","[4,   100] loss: 94.058 acc: 47.59 time: 37.99\n","Accuracy of the network on the test images: 65.22 %\n","[5,   100] loss: 91.209 acc: 51.50 time: 36.95\n","Accuracy of the network on the test images: 66.13 %\n","[6,   100] loss: 86.170 acc: 55.22 time: 35.96\n","Accuracy of the network on the test images: 68.90 %\n","[7,   100] loss: 85.713 acc: 55.28 time: 38.06\n","Accuracy of the network on the test images: 70.31 %\n","[8,   100] loss: 84.338 acc: 53.94 time: 36.31\n","Accuracy of the network on the test images: 70.20 %\n","[9,   100] loss: 80.825 acc: 59.19 time: 36.06\n","Accuracy of the network on the test images: 72.13 %\n","[10,   100] loss: 82.099 acc: 59.72 time: 37.69\n","Accuracy of the network on the test images: 72.32 %\n","[11,   100] loss: 76.993 acc: 61.44 time: 36.56\n","Accuracy of the network on the test images: 73.23 %\n","[12,   100] loss: 78.076 acc: 60.25 time: 37.31\n","Accuracy of the network on the test images: 73.43 %\n","[13,   100] loss: 78.874 acc: 61.53 time: 39.16\n","Accuracy of the network on the test images: 73.35 %\n","[14,   100] loss: 75.930 acc: 63.34 time: 37.56\n","Accuracy of the network on the test images: 74.33 %\n","[15,   100] loss: 76.439 acc: 60.25 time: 37.74\n","Accuracy of the network on the test images: 74.69 %\n","[16,   100] loss: 75.822 acc: 63.78 time: 39.82\n","Accuracy of the network on the test images: 74.94 %\n","[17,   100] loss: 77.900 acc: 62.00 time: 37.65\n","Accuracy of the network on the test images: 75.08 %\n","[18,   100] loss: 74.926 acc: 62.72 time: 36.71\n","Accuracy of the network on the test images: 75.24 %\n","[19,   100] loss: 76.696 acc: 61.28 time: 38.11\n","Accuracy of the network on the test images: 75.92 %\n","[20,   100] loss: 75.216 acc: 62.97 time: 35.80\n","Accuracy of the network on the test images: 75.68 %\n","[21,   100] loss: 74.578 acc: 65.97 time: 36.26\n","Accuracy of the network on the test images: 75.67 %\n","[22,   100] loss: 71.692 acc: 66.66 time: 39.37\n","Accuracy of the network on the test images: 76.27 %\n","[23,   100] loss: 73.911 acc: 64.19 time: 37.33\n","Accuracy of the network on the test images: 76.18 %\n","[24,   100] loss: 73.059 acc: 66.28 time: 37.33\n","Accuracy of the network on the test images: 76.66 %\n","[25,   100] loss: 70.206 acc: 68.16 time: 38.95\n","Accuracy of the network on the test images: 76.39 %\n"]}],"source":["train(net, criterion, optimizer, num_epochs=num_epochs)\n","\n","save_dir = os.path.join(ckpt_dir, 'cutmix.pt')\n","torch.save(net.state_dict(), save_dir)"]},{"cell_type":"code","source":[],"metadata":{"id":"1bhpJv2SiOfC"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1NcIsYDOhdXf1BGdVQTL3oP0Kqc-9Xxal","timestamp":1681704052349},{"file_id":"1tnNexWUhj0CnOK410etU-i1g_ENvr5kn","timestamp":1681615045876}],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}