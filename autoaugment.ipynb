{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33123,"status":"ok","timestamp":1683832302912,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"},"user_tz":300},"id":"-YPt63OC7mRq","outputId":"724d3d37-05c3-4776-bba9-8d9ff8288d41"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"vRdwFtuzXtgl","executionInfo":{"status":"ok","timestamp":1683832307575,"user_tz":300,"elapsed":4666,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}}},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","import time\n","import tqdm\n","\n","import torch.nn as nn\n","import torchvision.models as  models\n","from torch.utils.data import DataLoader\n","import torchvision.datasets.voc as voc\n","import torch.optim as optim\n","from torchvision import transforms\n","from torchvision.models import resnet18\n","import torch.utils.model_zoo as model_zoo"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1683832307575,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"},"user_tz":300},"id":"aQ22btbUZu8E","outputId":"ff11b73a-4b1e-4f84-e2b0-653a327c09e3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f7490302410>"]},"metadata":{},"execution_count":3}],"source":["data_dir = '/content/drive/MyDrive/CS 444: DL for CV/Project/data'\n","ckpt_dir = '/content/drive/MyDrive/CS 444: DL for CV/Project/checkpoints'\n","object_categories = ['aeroplane', 'bicycle', 'bird', 'boat',\n","                     'bottle', 'bus', 'car', 'cat', 'chair',\n","                     'cow', 'diningtable', 'dog', 'horse',\n","                     'motorbike', 'person', 'pottedplant',\n","                     'sheep', 'sofa', 'train', 'tvmonitor']\n","num_classes = len(object_categories)\n","batch_size = 32\n","resnet_lr = 1e-4\n","fc_lr = 5e-3\n","num_epochs = 35\n","\n","mean = [0.457342265910642, 0.4387686270106377, 0.4073427106250871]\n","std = [0.26753769276329037, 0.2638145880487105, 0.2776826934044154]\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","np.random.seed(1902)\n","torch.manual_seed(1902)"]},{"cell_type":"markdown","metadata":{"id":"wJ-uygocbOw8"},"source":["## Data Pipeline\n","\n","Download the PASCAL VOC dataset and create train and val data loaders."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"bnu_0d_VY3_5","executionInfo":{"status":"ok","timestamp":1683832307575,"user_tz":300,"elapsed":1,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}}},"outputs":[],"source":["class PascalVOC_Dataset(voc.VOCDetection):\n","    \"\"\"Pascal VOC Detection Dataset\"\"\"\n","    def __init__(self, root, image_set='train', download=False, transform=None, target_transform=None):\n","        super().__init__(root, image_set=image_set, download=download, transform=transform, target_transform=target_transform)\n","    \n","    def __getitem__(self, index):\n","        return super().__getitem__(index)\n","    \n","    def __len__(self):\n","        return len(self.images)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"6qNEJbp6aCjE","executionInfo":{"status":"ok","timestamp":1683832307912,"user_tz":300,"elapsed":338,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}}},"outputs":[],"source":["def encode_labels(target):\n","    \"\"\"Encode multiple labels using 1/0 encoding\"\"\"\n","    ls = target['annotation']['object']\n","    j = []\n","    if type(ls) == dict:\n","        if int(ls['difficult']) == 0:\n","            j.append(object_categories.index(ls['name']))\n","    else:\n","        for i in range(len(ls)):\n","            if int(ls[i]['difficult']) == 0:\n","                j.append(object_categories.index(ls[i]['name']))\n","    k = np.zeros(len(object_categories))\n","    k[j] = 1\n","    return torch.from_numpy(k)"]},{"cell_type":"code","source":["transformations = transforms.Compose([transforms.Resize((300, 300)),\n","                                      transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),\n","                                      transforms.ToTensor(), \n","                                      transforms.Normalize(mean=mean, std=std)])\n","transformations_valid = transforms.Compose([transforms.Resize(330), \n","                                            transforms.CenterCrop(300), \n","                                            transforms.ToTensor(), \n","                                            transforms.Normalize(mean=mean, std=std)])"],"metadata":{"id":"MqL9Sm0cx-kd","executionInfo":{"status":"ok","timestamp":1683832340847,"user_tz":300,"elapsed":1,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"qR20ICwYZfib","executionInfo":{"status":"ok","timestamp":1683832351115,"user_tz":300,"elapsed":5226,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}}},"outputs":[],"source":["dataset_train = PascalVOC_Dataset(data_dir,\n","                                  image_set='train', \n","                                  download=False, \n","                                  transform=transformations, \n","                                  target_transform=encode_labels)\n","train_loader = DataLoader(dataset_train, batch_size=batch_size, num_workers=2, shuffle=True)\n","\n","dataset_valid = PascalVOC_Dataset(data_dir, \n","                                  image_set='val', \n","                                  download=False, \n","                                  transform=transformations_valid, \n","                                  target_transform=encode_labels)\n","valid_loader = DataLoader(dataset_valid, batch_size=batch_size, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"1NNfmBcIbaif"},"source":["## Define Model"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ZIcFhPiqbOBR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683832358240,"user_tz":300,"elapsed":5069,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}},"outputId":"742a09bd-4979-45cd-e87f-9f3752e4b971"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 353MB/s]\n"]}],"source":["net = resnet18(pretrained=True)\n","net.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n","num_ftrs = net.fc.in_features\n","net.fc = torch.nn.Linear(num_ftrs, num_classes)\n","net = net.to(device)"]},{"cell_type":"markdown","metadata":{"id":"hSuc51ptcBFZ"},"source":["## Define Training Parameters"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"d0qXwNxrcDNa","executionInfo":{"status":"ok","timestamp":1683832358241,"user_tz":300,"elapsed":5,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}}},"outputs":[],"source":["optimizer = optim.SGD([{'params': list(net.parameters())[:-1], 'lr': resnet_lr, 'momentum': 0.9},\n","                       {'params': list(net.parameters())[-1], 'lr': fc_lr, 'momentum': 0.9}])\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 12, eta_min=0, last_epoch=-1)\n","criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Tru2BAex5DNe","executionInfo":{"status":"ok","timestamp":1683832358241,"user_tz":300,"elapsed":3,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}}},"outputs":[],"source":["def run_test(net, test_loader, criterion):\n","    correct = 0\n","    total = 0\n","    avg_test_loss = 0.0\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            \n","            outputs = net(images)\n","            predictions = torch.argmax(outputs, dim=1)\n","            labels = torch.argmax(labels, dim=1)\n","            correct += torch.sum(predictions == labels)\n","            total += labels.size(0)\n","\n","    print(f'Accuracy of the network on the test images: {100 * correct / total:.2f} %')"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"eZGfBjDW4b9r","executionInfo":{"status":"ok","timestamp":1683832358241,"user_tz":300,"elapsed":3,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}}},"outputs":[],"source":["def train(net, criterion, optimizer, num_epochs, print_freq=100):\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        running_correct = 0.0\n","        running_total = 0.0\n","        start_time = time.time()\n","\n","        net.train()\n","\n","        for i, (images, labels) in enumerate(train_loader, 0):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = net(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","            # Get predicted results\n","            predicted = torch.argmax(outputs, dim=1)\n","            labels = torch.argmax(labels, dim=1)\n","\n","            # print statistics\n","            running_loss += loss.item()\n","\n","            # calculate accuracy\n","            running_total += labels.size(0)\n","            running_correct += (predicted == labels).sum().item()\n","\n","            # print every 2000 mini-batches\n","            if i % print_freq == (print_freq - 1):\n","                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / print_freq:.3f} acc: {100*running_correct / running_total:.2f} time: {time.time() - start_time:.2f}')\n","                running_loss, running_correct, running_total = 0.0, 0.0, 0.0\n","                start_time = time.time()\n","\n","        # Run the run_test() function after each epoch\n","        net.eval()\n","        run_test(net, valid_loader, criterion)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"f8gpDFo_BCNN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b1c991c7-5186-4906-de29-b26d4700bcb8","executionInfo":{"status":"ok","timestamp":1683846521805,"user_tz":300,"elapsed":14155725,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[1,   100] loss: 127.022 acc: 35.84 time: 2753.53\n","Accuracy of the network on the test images: 68.83 %\n","[2,   100] loss: 66.279 acc: 64.31 time: 42.16\n","Accuracy of the network on the test images: 71.10 %\n","[3,   100] loss: 55.791 acc: 67.28 time: 43.15\n","Accuracy of the network on the test images: 69.88 %\n","[4,   100] loss: 49.486 acc: 69.25 time: 42.90\n","Accuracy of the network on the test images: 71.60 %\n","[5,   100] loss: 45.251 acc: 71.22 time: 43.66\n","Accuracy of the network on the test images: 70.69 %\n","[6,   100] loss: 38.967 acc: 72.81 time: 43.46\n","Accuracy of the network on the test images: 72.14 %\n","[7,   100] loss: 34.889 acc: 73.09 time: 43.37\n","Accuracy of the network on the test images: 71.06 %\n","[8,   100] loss: 29.837 acc: 75.56 time: 42.95\n","Accuracy of the network on the test images: 71.53 %\n","[9,   100] loss: 26.737 acc: 76.16 time: 43.04\n","Accuracy of the network on the test images: 69.48 %\n","[10,   100] loss: 25.455 acc: 75.06 time: 42.97\n","Accuracy of the network on the test images: 71.75 %\n","[11,   100] loss: 22.301 acc: 76.97 time: 42.91\n","Accuracy of the network on the test images: 68.21 %\n","[12,   100] loss: 20.649 acc: 77.00 time: 42.92\n","Accuracy of the network on the test images: 70.03 %\n","[13,   100] loss: 19.285 acc: 74.84 time: 43.05\n","Accuracy of the network on the test images: 70.84 %\n","[14,   100] loss: 16.289 acc: 78.28 time: 42.61\n","Accuracy of the network on the test images: 69.59 %\n","[15,   100] loss: 16.331 acc: 77.12 time: 42.73\n","Accuracy of the network on the test images: 71.77 %\n","[16,   100] loss: 14.504 acc: 77.72 time: 42.15\n","Accuracy of the network on the test images: 68.18 %\n","[17,   100] loss: 14.144 acc: 78.06 time: 42.50\n","Accuracy of the network on the test images: 70.67 %\n","[18,   100] loss: 13.756 acc: 76.84 time: 42.09\n","Accuracy of the network on the test images: 67.66 %\n","[19,   100] loss: 11.454 acc: 79.47 time: 42.25\n","Accuracy of the network on the test images: 70.74 %\n","[20,   100] loss: 11.251 acc: 77.31 time: 41.84\n","Accuracy of the network on the test images: 69.41 %\n","[21,   100] loss: 11.013 acc: 79.56 time: 42.36\n","Accuracy of the network on the test images: 71.87 %\n","[22,   100] loss: 11.717 acc: 76.75 time: 42.31\n","Accuracy of the network on the test images: 71.53 %\n","[23,   100] loss: 10.803 acc: 77.44 time: 41.86\n","Accuracy of the network on the test images: 70.75 %\n","[24,   100] loss: 9.628 acc: 77.78 time: 40.64\n","Accuracy of the network on the test images: 68.54 %\n","[25,   100] loss: 9.228 acc: 78.59 time: 41.57\n","Accuracy of the network on the test images: 71.17 %\n","[26,   100] loss: 9.082 acc: 77.25 time: 41.81\n","Accuracy of the network on the test images: 70.60 %\n","[27,   100] loss: 9.100 acc: 77.88 time: 41.95\n","Accuracy of the network on the test images: 69.83 %\n","[28,   100] loss: 8.251 acc: 78.94 time: 41.89\n","Accuracy of the network on the test images: 70.07 %\n","[29,   100] loss: 8.551 acc: 79.91 time: 41.86\n","Accuracy of the network on the test images: 70.07 %\n","[30,   100] loss: 7.331 acc: 78.19 time: 41.79\n","Accuracy of the network on the test images: 69.38 %\n","[31,   100] loss: 8.423 acc: 77.94 time: 41.80\n","Accuracy of the network on the test images: 68.62 %\n","[32,   100] loss: 8.263 acc: 79.19 time: 41.62\n","Accuracy of the network on the test images: 71.68 %\n","[33,   100] loss: 6.241 acc: 78.94 time: 41.88\n","Accuracy of the network on the test images: 71.35 %\n","[34,   100] loss: 6.745 acc: 77.81 time: 41.64\n","Accuracy of the network on the test images: 71.18 %\n","[35,   100] loss: 6.688 acc: 78.66 time: 41.83\n","Accuracy of the network on the test images: 68.68 %\n"]}],"source":["train(net, criterion, optimizer, num_epochs=num_epochs)\n","\n","save_dir = os.path.join(ckpt_dir, 'autoaugment.pt')\n","torch.save(net.state_dict(), save_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1bhpJv2SiOfC"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1tnNexWUhj0CnOK410etU-i1g_ENvr5kn","timestamp":1681614775494}],"gpuType":"T4"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}