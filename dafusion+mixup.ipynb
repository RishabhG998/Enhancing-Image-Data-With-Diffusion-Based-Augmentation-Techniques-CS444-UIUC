{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"-YPt63OC7mRq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684026742414,"user_tz":300,"elapsed":22291,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}},"outputId":"e116e94c-c128-4e1a-afc8-3325f5d718d1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":11,"metadata":{"id":"vRdwFtuzXtgl","executionInfo":{"status":"ok","timestamp":1684027104867,"user_tz":300,"elapsed":464,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}}},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","import time\n","from typing import Tuple\n","import tqdm\n","import shutil\n","import torch.nn as nn\n","import torchvision.models as  models\n","from torch.utils.data import DataLoader, ConcatDataset\n","import torchvision.datasets.voc as voc\n","import torch.optim as optim\n","from PIL import Image\n","from glob import glob\n","from tqdm import tqdm\n","from torchvision import transforms\n","from torchvision.models import resnet18\n","import torch.utils.model_zoo as model_zoo\n","import xml.etree.cElementTree as ET"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":393,"status":"ok","timestamp":1684026821633,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"},"user_tz":300},"id":"aQ22btbUZu8E","outputId":"951a9f28-be78-4bf2-a91b-2868b7c3a3cf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f2aec632250>"]},"metadata":{},"execution_count":3}],"source":["data_dir = '/content/drive/MyDrive/CS 444: DL for CV/Project/data'\n","aug_dir = '/content/drive/MyDrive/CS 444: DL for CV/Project/data/pascal-aug/pascal-0-8'\n","ckpt_dir = '/content/drive/MyDrive/CS 444: DL for CV/Project/checkpoints'\n","object_categories = ['aeroplane', 'bicycle', 'bird', 'boat',\n","                     'bottle', 'bus', 'car', 'cat', 'chair',\n","                     'cow', 'diningtable', 'dog', 'horse',\n","                     'motorbike', 'person', 'pottedplant',\n","                     'sheep', 'sofa', 'train', 'tvmonitor']\n","num_classes = len(object_categories)\n","batch_size = 32\n","resnet_lr = 1e-5\n","fc_lr = 5e-3\n","num_epochs = 25\n","\n","mean = [0.457342265910642, 0.4387686270106377, 0.4073427106250871]\n","std = [0.26753769276329037, 0.2638145880487105, 0.2776826934044154]\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","np.random.seed(1902)\n","torch.manual_seed(1902)"]},{"cell_type":"markdown","metadata":{"id":"wJ-uygocbOw8"},"source":["## Data Pipeline\n","\n","Download the PASCAL VOC dataset and create train and val data loaders."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"bnu_0d_VY3_5","executionInfo":{"status":"ok","timestamp":1684026830063,"user_tz":300,"elapsed":1,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}}},"outputs":[],"source":["class PascalVOC_Dataset(voc.VOCDetection):\n","    \"\"\"Pascal VOC Detection Dataset\"\"\"\n","    def __init__(self, root, image_set='train', download=False, transform=None, target_transform=None):\n","        super().__init__(root, image_set=image_set, download=download, transform=transform, target_transform=target_transform)\n","    \n","    def __getitem__(self, index):\n","        return super().__getitem__(index)\n","    \n","    def __len__(self):\n","        return len(self.images)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"6qNEJbp6aCjE","executionInfo":{"status":"ok","timestamp":1684026831669,"user_tz":300,"elapsed":1,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}}},"outputs":[],"source":["def encode_labels(target):\n","    \"\"\"Encode multiple labels using 1/0 encoding\"\"\"\n","    ls = target['annotation']['object']\n","    j = []\n","    if type(ls) == dict:\n","        if int(ls['difficult']) == 0:\n","            j.append(object_categories.index(ls['name']))\n","    else:\n","        for i in range(len(ls)):\n","            if int(ls[i]['difficult']) == 0:\n","                j.append(object_categories.index(ls[i]['name']))\n","    k = np.zeros(len(object_categories))\n","    k[j] = 1\n","    return torch.from_numpy(k)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"2MycNoV8cwq_","executionInfo":{"status":"ok","timestamp":1684026852772,"user_tz":300,"elapsed":466,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}}},"outputs":[],"source":["transformations = transforms.Compose([transforms.Resize((300, 300)),\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize(mean=mean, std=std)])\n","transformations_valid = transforms.Compose([transforms.Resize(330), \n","                                            transforms.CenterCrop(300), \n","                                            transforms.ToTensor(),\n","                                            transforms.Normalize(mean=mean, std=std)])"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"qR20ICwYZfib","executionInfo":{"status":"ok","timestamp":1684027046927,"user_tz":300,"elapsed":778,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}}},"outputs":[],"source":["dataset_train = PascalVOC_Dataset(data_dir,\n","                                  image_set='train', \n","                                  download=False, \n","                                  transform=transformations, \n","                                  target_transform=encode_labels)\n","dataset_aug = PascalVOC_Dataset(aug_dir,\n","                                image_set='train', \n","                                download=False, \n","                                transform=transformations, \n","                                target_transform=encode_labels)\n","dataset_combined = ConcatDataset([dataset_train, dataset_aug])\n","train_loader = DataLoader(dataset_combined, batch_size=batch_size, num_workers=2, shuffle=True)\n","\n","dataset_valid = PascalVOC_Dataset(data_dir, \n","                                  image_set='val', \n","                                  download=False, \n","                                  transform=transformations_valid, \n","                                  target_transform=encode_labels)\n","valid_loader = DataLoader(dataset_valid, batch_size=batch_size, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"1NNfmBcIbaif"},"source":["## Define Model"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"ZIcFhPiqbOBR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684027071999,"user_tz":300,"elapsed":5217,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}},"outputId":"02d0f192-bc95-4d75-c7a9-3065ac3c54fe"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 304MB/s]\n"]}],"source":["net = resnet18(pretrained=True)\n","net.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n","num_ftrs = net.fc.in_features\n","net.fc = torch.nn.Linear(num_ftrs, num_classes)\n","net = net.to(device)"]},{"cell_type":"code","source":["class RandomMixup(torch.nn.Module):\n","    \"\"\"Randomly apply Mixup to the provided batch and targets.\n","    The class implements the data augmentations as described in the paper\n","    `\"mixup: Beyond Empirical Risk Minimization\" <https://arxiv.org/abs/1710.09412>`_.\n","    Args:\n","        num_classes (int): number of classes used for one-hot encoding.\n","        p (float): probability of the batch being transformed. Default value is 0.5.\n","        alpha (float): hyperparameter of the Beta distribution used for mixup.\n","            Default value is 1.0.\n","        inplace (bool): boolean to make this transform inplace. Default set to False.\n","    \"\"\"\n","\n","    def __init__(self, num_classes: int, p: float = 0.5, alpha: float = 1.0, inplace: bool = False) -> None:\n","        super().__init__()\n","\n","        if num_classes < 1:\n","            raise ValueError(\n","                f\"Please provide a valid positive value for the num_classes. Got num_classes={num_classes}\"\n","            )\n","\n","        if alpha <= 0:\n","            raise ValueError(\"Alpha param can't be zero.\")\n","\n","        self.num_classes = num_classes\n","        self.p = p\n","        self.alpha = alpha\n","        self.inplace = inplace\n","\n","    def forward(self, batch: torch.Tensor, target: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        Args:\n","            batch (Tensor): Float tensor of size (B, C, H, W)\n","            target (Tensor): Integer tensor of size (B, )\n","        Returns:\n","            Tensor: Randomly transformed batch.\n","        \"\"\"\n","        if batch.ndim != 4:\n","            raise ValueError(f\"Batch ndim should be 4. Got {batch.ndim}\")\n","        if target.ndim != 1:\n","            raise ValueError(f\"Target ndim should be 1. Got {target.ndim}\")\n","        if not batch.is_floating_point():\n","            raise TypeError(f\"Batch dtype should be a float tensor. Got {batch.dtype}.\")\n","        if target.dtype != torch.int64:\n","            raise TypeError(f\"Target dtype should be torch.int64. Got {target.dtype}\")\n","\n","        if not self.inplace:\n","            batch = batch.clone()\n","            target = target.clone()\n","\n","        if target.ndim == 1:\n","            target = torch.nn.functional.one_hot(target, num_classes=self.num_classes).to(dtype=batch.dtype)\n","\n","        if torch.rand(1).item() >= self.p:\n","            return batch, target\n","\n","        batch_rolled = batch.roll(1, 0)\n","        target_rolled = target.roll(1, 0)\n","\n","        lambda_param = float(torch._sample_dirichlet(torch.tensor([self.alpha, self.alpha]))[0])\n","        batch_rolled.mul_(1.0 - lambda_param)\n","        batch.mul_(lambda_param).add_(batch_rolled)\n","\n","        target_rolled.mul_(1.0 - lambda_param)\n","        target.mul_(lambda_param).add_(target_rolled)\n","\n","        return batch, target\n","\n","    def __repr__(self) -> str:\n","        s = (\n","            f\"{self.__class__.__name__}(\"\n","            f\"num_classes={self.num_classes}\"\n","            f\", p={self.p}\"\n","            f\", alpha={self.alpha}\"\n","            f\", inplace={self.inplace}\"\n","            f\")\"\n","        )\n","        return s"],"metadata":{"id":"M5EZk4Hs1cPR","executionInfo":{"status":"ok","timestamp":1684027107715,"user_tz":300,"elapsed":443,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hSuc51ptcBFZ"},"source":["## Define Training Parameters"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"d0qXwNxrcDNa","executionInfo":{"status":"ok","timestamp":1684027111967,"user_tz":300,"elapsed":412,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}}},"outputs":[],"source":["optimizer = optim.SGD([{'params': list(net.parameters())[:-1], 'lr': resnet_lr, 'momentum': 0.9},\n","                       {'params': list(net.parameters())[-1], 'lr': fc_lr, 'momentum': 0.9}])\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 12, eta_min=0, last_epoch=-1)\n","criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Tru2BAex5DNe","executionInfo":{"status":"ok","timestamp":1684027113548,"user_tz":300,"elapsed":1,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}}},"outputs":[],"source":["def run_test(net, test_loader, criterion):\n","    correct = 0\n","    total = 0\n","    avg_test_loss = 0.0\n","    l = len(test_loader)\n","    with torch.no_grad():\n","        for _, (images, labels) in enumerate(test_loader):\n","            images, labels = images.to(device), labels.to(device)\n","            \n","            outputs = net(images)\n","            predictions = torch.argmax(outputs, dim=1)\n","            labels = torch.argmax(labels, dim=1)\n","            correct += torch.sum(predictions == labels)\n","            total += labels.size(0)\n","\n","    print(f'Accuracy of the network on the test images: {100 * correct / total:.2f} %')"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"eZGfBjDW4b9r","executionInfo":{"status":"ok","timestamp":1684027136868,"user_tz":300,"elapsed":395,"user":{"displayName":"Savya Khosla","userId":"05745142373296942590"}}},"outputs":[],"source":["def train(net, criterion, optimizer, num_epochs, print_freq = 100):\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        running_correct = 0.0\n","        running_total = 0.0\n","        start_time = time.time()\n","\n","        net.train()\n","\n","        for i, (images, labels) in enumerate(train_loader, 0):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            images, labels = RandomMixup(num_classes, p=1, alpha=0.5)(images, torch.argmax(labels, dim=1))\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = net(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","            # Get predicted results\n","            predicted = torch.argmax(outputs, dim=1)\n","            labels = torch.argmax(labels, dim=1)\n","\n","            # print statistics\n","            running_loss += loss.item()\n","\n","            # calculate accuracy\n","            running_total += labels.size(0)\n","            running_correct += (predicted == labels).sum().item()\n","\n","            # print every 2000 mini-batches\n","            if i % print_freq == (print_freq - 1):\n","                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / print_freq:.3f} acc: {100*running_correct / running_total:.2f} time: {time.time() - start_time:.2f}')\n","                running_loss, running_correct, running_total = 0.0, 0.0, 0.0\n","                start_time = time.time()\n","\n","        # Run the run_test() function after each epoch\n","        net.eval()\n","        run_test(net, valid_loader, criterion)"]},{"cell_type":"code","source":["train(net, criterion, optimizer, num_epochs=num_epochs)\n","\n","save_dir = os.path.join(ckpt_dir, 'da-mixup.pt')\n","torch.save(net.state_dict(), save_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2SfmfdZCh3Nm","outputId":"6b4a6227-1f66-480d-db14-25fe69f1b4d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1,   100] loss: 139.169 acc: 13.09 time: 2399.85\n","[1,   200] loss: 113.493 acc: 27.75 time: 2338.88\n","Accuracy of the network on the test images: 45.06 %\n","[2,   100] loss: 103.476 acc: 42.69 time: 38.84\n","[2,   200] loss: 99.242 acc: 47.00 time: 39.40\n","Accuracy of the network on the test images: 62.96 %\n","[3,   100] loss: 89.870 acc: 57.03 time: 38.52\n","[3,   200] loss: 87.326 acc: 60.56 time: 39.90\n","Accuracy of the network on the test images: 68.07 %\n","[4,   100] loss: 84.169 acc: 62.16 time: 40.46\n","[4,   200] loss: 83.153 acc: 62.16 time: 38.42\n","Accuracy of the network on the test images: 70.63 %\n","[5,   100] loss: 80.656 acc: 64.69 time: 40.19\n","[5,   200] loss: 81.052 acc: 62.81 time: 39.52\n","Accuracy of the network on the test images: 71.65 %\n","[6,   100] loss: 74.210 acc: 68.59 time: 39.66\n","[6,   200] loss: 76.421 acc: 66.06 time: 39.54\n","Accuracy of the network on the test images: 72.56 %\n","[7,   100] loss: 73.210 acc: 68.31 time: 39.96\n","[7,   200] loss: 76.699 acc: 66.88 time: 39.62\n","Accuracy of the network on the test images: 74.05 %\n","[8,   100] loss: 70.912 acc: 69.97 time: 38.51\n","[8,   200] loss: 71.869 acc: 70.81 time: 39.62\n","Accuracy of the network on the test images: 74.50 %\n","[9,   100] loss: 71.414 acc: 71.22 time: 40.23\n","[9,   200] loss: 69.983 acc: 70.81 time: 38.11\n","Accuracy of the network on the test images: 74.53 %\n","[10,   100] loss: 69.059 acc: 71.25 time: 40.04\n","[10,   200] loss: 69.829 acc: 70.41 time: 38.74\n","Accuracy of the network on the test images: 75.20 %\n","[11,   100] loss: 70.542 acc: 71.59 time: 40.40\n","[11,   200] loss: 69.170 acc: 72.59 time: 39.90\n","Accuracy of the network on the test images: 75.72 %\n","[12,   100] loss: 72.658 acc: 69.94 time: 40.22\n","[12,   200] loss: 67.520 acc: 73.94 time: 39.77\n","Accuracy of the network on the test images: 75.65 %\n","[13,   100] loss: 65.011 acc: 73.22 time: 39.79\n","[13,   200] loss: 65.550 acc: 76.41 time: 39.51\n","Accuracy of the network on the test images: 76.22 %\n","[14,   100] loss: 64.440 acc: 76.81 time: 40.23\n","[14,   200] loss: 66.240 acc: 74.66 time: 38.97\n","Accuracy of the network on the test images: 76.20 %\n","[15,   100] loss: 63.170 acc: 74.31 time: 40.23\n","[15,   200] loss: 65.586 acc: 74.53 time: 39.63\n","Accuracy of the network on the test images: 76.35 %\n","[16,   100] loss: 68.269 acc: 73.06 time: 40.09\n","[16,   200] loss: 67.877 acc: 73.56 time: 39.55\n","Accuracy of the network on the test images: 76.76 %\n","[17,   100] loss: 63.078 acc: 75.81 time: 40.00\n","[17,   200] loss: 67.480 acc: 76.12 time: 40.09\n","Accuracy of the network on the test images: 77.13 %\n","[18,   100] loss: 64.202 acc: 75.09 time: 38.23\n","[18,   200] loss: 61.106 acc: 77.47 time: 39.43\n","Accuracy of the network on the test images: 76.71 %\n","[19,   100] loss: 64.118 acc: 76.38 time: 40.54\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"9DYMrbTH11vL"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1pZPXfvG2l2iET5O68ELSbAsRt47V7C9x","timestamp":1684026625534},{"file_id":"1tnNexWUhj0CnOK410etU-i1g_ENvr5kn","timestamp":1683851527075}],"gpuType":"T4"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}