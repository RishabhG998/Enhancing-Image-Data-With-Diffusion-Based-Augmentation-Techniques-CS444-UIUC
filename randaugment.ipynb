{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21089,"status":"ok","timestamp":1683774458172,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"},"user_tz":300},"id":"-YPt63OC7mRq","outputId":"e382e8bc-b68b-4074-e01e-13109f091996"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vRdwFtuzXtgl"},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","import time\n","import tqdm\n","\n","import torch.nn as nn\n","import torchvision.models as  models\n","from torch.utils.data import DataLoader\n","import torchvision.datasets.voc as voc\n","import torch.optim as optim\n","from torchvision import transforms\n","from torchvision.models import resnet18\n","import torch.utils.model_zoo as model_zoo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":290,"status":"ok","timestamp":1683784120174,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"},"user_tz":300},"id":"aQ22btbUZu8E","outputId":"28ede0be-f33d-40c5-c31a-ceb20ea217a2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f2b1c1460b0>"]},"metadata":{},"execution_count":17}],"source":["data_dir = '/content/drive/MyDrive/CS 444: DL for CV/Project/data'\n","ckpt_dir = '/content/drive/MyDrive/CS 444: DL for CV/Project/checkpoints'\n","object_categories = ['aeroplane', 'bicycle', 'bird', 'boat',\n","                     'bottle', 'bus', 'car', 'cat', 'chair',\n","                     'cow', 'diningtable', 'dog', 'horse',\n","                     'motorbike', 'person', 'pottedplant',\n","                     'sheep', 'sofa', 'train', 'tvmonitor']\n","num_classes = len(object_categories)\n","batch_size = 32\n","resnet_lr = 1e-4\n","fc_lr = 5e-3\n","num_epochs = 35\n","\n","mean = [0.457342265910642, 0.4387686270106377, 0.4073427106250871]\n","std = [0.26753769276329037, 0.2638145880487105, 0.2776826934044154]\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","np.random.seed(1902)\n","torch.manual_seed(1902)"]},{"cell_type":"markdown","metadata":{"id":"wJ-uygocbOw8"},"source":["## Data Pipeline\n","\n","Download the PASCAL VOC dataset and create train and val data loaders."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bnu_0d_VY3_5"},"outputs":[],"source":["class PascalVOC_Dataset(voc.VOCDetection):\n","    \"\"\"Pascal VOC Detection Dataset\"\"\"\n","    def __init__(self, root, image_set='train', download=False, transform=None, target_transform=None):\n","        super().__init__(root, image_set=image_set, download=download, transform=transform, target_transform=target_transform)\n","    \n","    def __getitem__(self, index):\n","        return super().__getitem__(index)\n","    \n","    def __len__(self):\n","        return len(self.images)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qNEJbp6aCjE"},"outputs":[],"source":["def encode_labels(target):\n","    \"\"\"Encode multiple labels using 1/0 encoding\"\"\"\n","    ls = target['annotation']['object']\n","    j = []\n","    if type(ls) == dict:\n","        if int(ls['difficult']) == 0:\n","            j.append(object_categories.index(ls['name']))\n","    else:\n","        for i in range(len(ls)):\n","            if int(ls[i]['difficult']) == 0:\n","                j.append(object_categories.index(ls[i]['name']))\n","    k = np.zeros(len(object_categories))\n","    k[j] = 1\n","    return torch.from_numpy(k)"]},{"cell_type":"code","source":["transformations = transforms.Compose([transforms.Resize((300, 300)),\n","                                      transforms.RandAugment(num_ops=2, magnitude=1),\n","                                      transforms.ToTensor(), \n","                                      transforms.Normalize(mean=mean, std=std)])\n","transformations_valid = transforms.Compose([transforms.Resize(330), \n","                                            transforms.CenterCrop(300), \n","                                            transforms.ToTensor(), \n","                                            transforms.Normalize(mean=mean, std=std)])"],"metadata":{"id":"MqL9Sm0cx-kd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qR20ICwYZfib"},"outputs":[],"source":["dataset_train = PascalVOC_Dataset(data_dir,\n","                                  image_set='train', \n","                                  download=False, \n","                                  transform=transformations, \n","                                  target_transform=encode_labels)\n","train_loader = DataLoader(dataset_train, batch_size=batch_size, num_workers=2, shuffle=True)\n","\n","dataset_valid = PascalVOC_Dataset(data_dir, \n","                                  image_set='val', \n","                                  download=False, \n","                                  transform=transformations_valid, \n","                                  target_transform=encode_labels)\n","valid_loader = DataLoader(dataset_valid, batch_size=batch_size, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"1NNfmBcIbaif"},"source":["## Define Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZIcFhPiqbOBR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683784140725,"user_tz":300,"elapsed":628,"user":{"displayName":"Savya Khosla","userId":"02898378811354072740"}},"outputId":"496227c0-f4aa-4cd0-c097-7ba2bae39075"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}],"source":["net = resnet18(pretrained=True)\n","net.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n","num_ftrs = net.fc.in_features\n","net.fc = torch.nn.Linear(num_ftrs, num_classes)\n","net = net.to(device)"]},{"cell_type":"markdown","metadata":{"id":"hSuc51ptcBFZ"},"source":["## Define Training Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d0qXwNxrcDNa"},"outputs":[],"source":["optimizer = optim.SGD([{'params': list(net.parameters())[:-1], 'lr': resnet_lr, 'momentum': 0.9},\n","                       {'params': list(net.parameters())[-1], 'lr': fc_lr, 'momentum': 0.9}])\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 12, eta_min=0, last_epoch=-1)\n","criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tru2BAex5DNe"},"outputs":[],"source":["def run_test(net, test_loader, criterion):\n","    correct = 0\n","    total = 0\n","    avg_test_loss = 0.0\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            \n","            outputs = net(images)\n","            predictions = torch.argmax(outputs, dim=1)\n","            labels = torch.argmax(labels, dim=1)\n","            correct += torch.sum(predictions == labels)\n","            total += labels.size(0)\n","\n","    print(f'Accuracy of the network on the test images: {100 * correct / total:.2f} %')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eZGfBjDW4b9r"},"outputs":[],"source":["def train(net, criterion, optimizer, num_epochs, print_freq=100):\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        running_correct = 0.0\n","        running_total = 0.0\n","        start_time = time.time()\n","\n","        net.train()\n","\n","        for i, (images, labels) in enumerate(train_loader, 0):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = net(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","            # Get predicted results\n","            predicted = torch.argmax(outputs, dim=1)\n","            labels = torch.argmax(labels, dim=1)\n","\n","            # print statistics\n","            running_loss += loss.item()\n","\n","            # calculate accuracy\n","            running_total += labels.size(0)\n","            running_correct += (predicted == labels).sum().item()\n","\n","            # print every 2000 mini-batches\n","            if i % print_freq == (print_freq - 1):\n","                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / print_freq:.3f} acc: {100*running_correct / running_total:.2f} time: {time.time() - start_time:.2f}')\n","                running_loss, running_correct, running_total = 0.0, 0.0, 0.0\n","                start_time = time.time()\n","\n","        # Run the run_test() function after each epoch\n","        net.eval()\n","        run_test(net, valid_loader, criterion)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8gpDFo_BCNN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3f71865a-389a-4896-c314-6d1a2b7753f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["[1,   100] loss: 118.793 acc: 40.31 time: 45.30\n","Accuracy of the network on the test images: 69.35 %\n","[2,   100] loss: 56.686 acc: 68.66 time: 44.64\n","Accuracy of the network on the test images: 71.10 %\n","[3,   100] loss: 44.872 acc: 72.25 time: 44.29\n","Accuracy of the network on the test images: 71.03 %\n","[4,   100] loss: 36.931 acc: 73.69 time: 45.74\n","Accuracy of the network on the test images: 73.50 %\n","[5,   100] loss: 30.117 acc: 76.03 time: 45.60\n","Accuracy of the network on the test images: 71.84 %\n","[6,   100] loss: 23.668 acc: 77.81 time: 47.08\n","Accuracy of the network on the test images: 71.60 %\n","[7,   100] loss: 17.883 acc: 76.56 time: 45.91\n","Accuracy of the network on the test images: 69.19 %\n","[8,   100] loss: 13.984 acc: 78.44 time: 45.67\n","Accuracy of the network on the test images: 71.25 %\n","[9,   100] loss: 10.695 acc: 78.78 time: 45.96\n","Accuracy of the network on the test images: 71.44 %\n","[10,   100] loss: 8.986 acc: 77.78 time: 46.35\n","Accuracy of the network on the test images: 70.87 %\n","[11,   100] loss: 8.628 acc: 78.59 time: 47.09\n","Accuracy of the network on the test images: 70.94 %\n","[12,   100] loss: 6.243 acc: 79.28 time: 45.86\n","Accuracy of the network on the test images: 72.83 %\n","[13,   100] loss: 5.457 acc: 78.03 time: 45.82\n","Accuracy of the network on the test images: 72.08 %\n","[14,   100] loss: 4.583 acc: 79.38 time: 47.93\n","Accuracy of the network on the test images: 71.17 %\n","[15,   100] loss: 4.258 acc: 78.25 time: 46.10\n","Accuracy of the network on the test images: 71.82 %\n","[16,   100] loss: 3.369 acc: 77.72 time: 46.47\n","Accuracy of the network on the test images: 72.02 %\n","[17,   100] loss: 2.700 acc: 78.84 time: 48.03\n","Accuracy of the network on the test images: 70.93 %\n","[18,   100] loss: 3.144 acc: 78.06 time: 46.34\n","Accuracy of the network on the test images: 69.98 %\n","[19,   100] loss: 2.489 acc: 79.16 time: 46.22\n","Accuracy of the network on the test images: 71.42 %\n","[20,   100] loss: 1.967 acc: 79.16 time: 47.05\n","Accuracy of the network on the test images: 71.30 %\n","[21,   100] loss: 2.140 acc: 80.34 time: 45.74\n","Accuracy of the network on the test images: 72.13 %\n","[22,   100] loss: 2.280 acc: 77.88 time: 47.28\n","Accuracy of the network on the test images: 70.86 %\n","[23,   100] loss: 2.162 acc: 77.94 time: 46.53\n","Accuracy of the network on the test images: 71.39 %\n","[24,   100] loss: 1.711 acc: 78.75 time: 45.92\n","Accuracy of the network on the test images: 70.86 %\n","[25,   100] loss: 1.840 acc: 79.03 time: 47.80\n","Accuracy of the network on the test images: 72.42 %\n","[26,   100] loss: 1.774 acc: 79.19 time: 46.56\n","Accuracy of the network on the test images: 71.46 %\n","[27,   100] loss: 1.425 acc: 78.28 time: 46.43\n","Accuracy of the network on the test images: 70.60 %\n","[28,   100] loss: 1.372 acc: 78.38 time: 47.58\n","Accuracy of the network on the test images: 71.22 %\n","[29,   100] loss: 1.172 acc: 78.78 time: 48.61\n","Accuracy of the network on the test images: 70.96 %\n"]}],"source":["train(net, criterion, optimizer, num_epochs=num_epochs)\n","\n","save_dir = os.path.join(ckpt_dir, 'randaugment.pt')\n","torch.save(net.state_dict(), save_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1bhpJv2SiOfC"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1tnNexWUhj0CnOK410etU-i1g_ENvr5kn","timestamp":1681614775494}],"gpuType":"T4"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}